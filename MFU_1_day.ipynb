{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 실습예제 1-1) Google Colab 실습 환경 안내"
      ],
      "metadata": {
        "id": "lK5VuxyMzC_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📚 Google Colab 안내서\n",
        "Google Colab(Colaboratory)은 **브라우저에서 Python/Jupyter 노트북을 실행**할 수 있는 구글의 무료 클라우드 서비스입니다.  \n",
        "GPU/TPU를 손쉽게 사용하고, 구글 드라이브와 연동하여 데이터/노트북을 관리할 수 있습니다.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 핵심 특징\n",
        "\n",
        "- **클라우드 Jupyter Notebook**: 로컬 설치 없이 바로 실행\n",
        "- **무료/유료 GPU·TPU 지원**: T4, L4, A100(유료) 등\n",
        "- **Google Drive 연동**: 파일 저장·공유 간편\n",
        "- **손쉬운 공유**: URL로 협업(보기/편집 권한 설정)\n",
        "- **풍부한 예제/라이브러리**: pip로 즉시 설치 가능\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 시작하기\n",
        "\n",
        "1. 접속: https://colab.research.google.com  \n",
        "2. `파일 → 새 노트` 또는 `GitHub/Drive`에서 노트 열기  \n",
        "3. `런타임 → 런타임 유형 변경 → 하드웨어 가속기: GPU/TPU` 선택\n",
        "\n",
        "---\n",
        "\n",
        "## 🧰 기본 사용법 & 유용한 코드\n",
        "\n",
        "### 1) 환경 확인\n"
      ],
      "metadata": {
        "id": "jlxY3-kOLB_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, torch, platform\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"PyTorch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdszDqQsHWyV",
        "outputId": "f950abf0-ec76-42eb-d810-8f01d89fa06f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "PyTorch: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-80GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FLOPs/MAC 계산 실습"
      ],
      "metadata": {
        "id": "fk_vxlkQzXXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 설치\n",
        "!pip install calflops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50xE55rFnbkl",
        "outputId": "c642b952-8607-4fbb-c264-72c9bbd3b6dd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: calflops in /usr/local/lib/python3.12/dist-packages (0.3.2)\n",
            "Requirement already satisfied: accelerate>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from calflops) (1.10.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from calflops) (0.35.3)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from calflops) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.22.0->calflops) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.22.0->calflops) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.22.0->calflops) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.22.0->calflops) (6.0.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.22.0->calflops) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.16.4->calflops) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.16.4->calflops) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.16.4->calflops) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.16.4->calflops) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.16.4->calflops) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.16.4->calflops) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.1.0->calflops) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.1.0->calflops) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.1.0->calflops) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.1.0->calflops) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.1.0->calflops) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.1.0->calflops) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.1.0->calflops) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.1.0->calflops) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.1.0->calflops) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.1.0->calflops) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.1.0->calflops) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.1.0->calflops) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.1.0->calflops) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.1.0->calflops) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.1.0->calflops) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.1.0->calflops) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.1.0->calflops) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.1.0->calflops) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.1.0->calflops) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.1.0->calflops) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.1.0->calflops) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.16.4->calflops) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.16.4->calflops) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.16.4->calflops) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.16.4->calflops) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqiMagy2nJyt",
        "outputId": "34af7e2f-e4b7-4383-d149-6025e8e04dda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 184MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLOPs: 3.64G, MACs: 1.81G\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from calflops import calculate_flops\n",
        "\n",
        "def parse_flop_string(s):\n",
        "    return float(s.strip().split()[0])\n",
        "\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', weights='ResNet18_Weights.DEFAULT')\n",
        "\n",
        "flops, macs, params = calculate_flops(\n",
        "    model=model,\n",
        "    input_shape=(1, 3, 224, 224),\n",
        "    print_results=False\n",
        ")\n",
        "\n",
        "print(f\"FLOPs: {parse_flop_string(flops):.2f}G, MACs: {parse_flop_string(macs):.2f}G\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 실습예제 1-2) GPU Utilization vs MFU 개념 및 실습 개요\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 학습 목표\n",
        "이 실습에서는 **GPU Utilization(활용률)** 과 **MFU(Model FLOPs Utilization)** 의 차이를 이해하고  \n",
        "직접 실험을 통해 두 지표가 서로 다른 의미를 가지는 이유를 관찰합니다.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 핵심 개념 비교\n",
        "\n",
        "| 항목 | GPU Utilization | MFU (Model FLOPs Utilization) |\n",
        "|------|----------------|------------------------------|\n",
        "| **정의** | GPU가 현재 얼마나 바쁘게 동작 중인지 (%) | GPU가 낼 수 있는 최대 연산 성능 대비 실제 모델이 활용한 비율 (%) |\n",
        "| **측정 방법** | `nvidia-smi` 명령어 등으로 실시간 사용률 확인 | FLOPs, 실행 시간, GPU 이론 성능을 기반으로 계산 |\n",
        "| **단위** | % | % |\n",
        "| **의미** | GPU가 일하는 시간의 비율 | GPU가 “얼마나 효율적으로” 일했는가 |\n",
        "| **주요 병목 요인** | 데이터 로딩, I/O, 동기화 지연 | 연산 최적화 부족, 배치 크기, 커널 효율 |\n",
        "| **활용 목적** | 시스템 상태 확인 | 모델 최적화 및 효율 분석 |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "SFJ3_tM4JMIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧩 실습 구성 단계\n",
        "\n",
        "1️⃣ **GPU 환경 확인**  \n",
        "현재 Colab 또는 로컬 환경에서 GPU 사용 가능 여부와 사양 확인"
      ],
      "metadata": {
        "id": "4q743uAjJWNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, time, os\n",
        "\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "    os.system(\"nvidia-smi | head -n 20\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNicaXeaJZ6A",
        "outputId": "f4cb597c-f6d9-41ee-fb71-6bc835e6010a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-80GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚙️ 2️⃣ 간단한 모델 정의 (Conv + FC)"
      ],
      "metadata": {
        "id": "PYZLFP4VJggr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(3, 32, 3, stride=2, padding=1)\n",
        "        self.fc = nn.Linear(32 * 112 * 112, 10)\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)\n",
        "\n",
        "model = SimpleCNN().cuda().eval()"
      ],
      "metadata": {
        "id": "9ISrRkvhJnwh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧮 3️⃣ FLOPs 계산 (모델 연산량)"
      ],
      "metadata": {
        "id": "YK46jthoJwdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FLOPs = 2 * H * W * Cin * Cout * Kh * Kw\n",
        "H, W, Cin, Cout, Kh, Kw = 224, 224, 3, 32, 3, 3\n",
        "flops_conv = 2 * H/2 * W/2 * Cin * Cout * Kh * Kw  # stride=2 → H/2,W/2\n",
        "flops_fc = 2 * (32 * 112 * 112) * 10\n",
        "total_flops = flops_conv + flops_fc\n",
        "print(f\"총 FLOPs: {total_flops/1e9:.3f} GFLOPs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L596ytmFJyme",
        "outputId": "4cc7e711-28c7-42e5-e181-73bc69a1ed76"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 FLOPs: 0.030 GFLOPs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "⏱️ 4️⃣ 실행 시간 측정 (Forward + Backward)"
      ],
      "metadata": {
        "id": "P8ARD_SzJ3YA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(32, 3, 224, 224).cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "y = torch.randint(0, 10, (32,)).cuda()\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "start = time.time()\n",
        "\n",
        "for _ in range(50):  # 50 iterations\n",
        "    optimizer.zero_grad()\n",
        "    out = model(x)\n",
        "    loss = criterion(out, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "end = time.time()\n",
        "train_time = (end - start) / 50\n",
        "print(f\"평균 반복당 학습 시간: {train_time:.4f} 초\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6AS9alcJ5Rp",
        "outputId": "50ef6ac7-d3b4-4c25-e511-ba82526213ac"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "평균 반복당 학습 시간: 0.0231 초\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5️⃣ 🧩 MFU 계산식\n",
        "\n",
        "$$\n",
        "MFU = \\frac{(\\text{모델의 총 FLOPs} / \\text{실행 시간})}{\\text{GPU의 이론적 FLOPs}} \\times 100\n",
        "$$\n",
        "\n",
        "- $\\text{모델의 총 FLOPs}$ : 모델이 한 번의 forward/backward에서 수행한 부동소수점 연산 수  \n",
        "- $\\text{GPU의 이론적 FLOPs}$ : GPU가 낼 수 있는 최대 부동소수점 연산량 (A100 기준 $19.5 \\times 10^{12}$ FLOPs/s)  \n",
        "- **결과 단위:** %"
      ],
      "metadata": {
        "id": "UCdgG4iHKBro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_theoretical_flops = 19.5e12  # A100 기준 (FP32)\n",
        "mfu = (total_flops / train_time) / gpu_theoretical_flops * 100\n",
        "print(f\"🔹 MFU(Model FLOPs Utilization): {mfu:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJxnhZTBKDxT",
        "outputId": "8ae5aea5-b6f3-40c0-b27d-9b7e05a39554"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 MFU(Model FLOPs Utilization): 0.01%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔎 6️⃣ GPU Utilization 확인\n",
        "\n",
        "아래 셀을 실행해 실시간 GPU 활용률을 확인합니다.\n",
        "(Colab에서는 watch 명령이 지원되지 않으므로 단발성 출력으로 확인합니다.)"
      ],
      "metadata": {
        "id": "Hq3E15c0LBsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi --query-gpu=utilization.gpu,utilization.memory,memory.used --format=csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5Yt0Y9ILEOY",
        "outputId": "dd26f8e4-0f01-476e-f3a8-8f2664d38675"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "utilization.gpu [%], utilization.memory [%], memory.used [MiB]\n",
            "0 %, 0 %, 809 MiB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "📈 7️⃣ 결과 비교\n",
        "\n",
        "| 지표                  | 의미                  | 계산 기준            | 예시 값    |\n",
        "| ------------------- | ------------------- | ---------------- | ------- |\n",
        "| **GPU Utilization** | GPU가 바쁘게 일한 비율      | 실시간 GPU 활용률(%)   | 예: 85 % |\n",
        "| **MFU**             | GPU의 이론 연산 대비 실제 효율 | FLOPs / 이론 FLOPs | 예: 25 % |\n",
        "\n",
        "➡️ GPU 활용률은 높더라도, 실제 연산 효율(MFU)은 낮을 수 있습니다.\n",
        "이는 데이터 로딩 지연, 메모리 대역폭 한계, 작은 배치 크기 등의 요인 때문입니다.\n",
        "\n",
        "\n",
        "## 📘 기대 학습 효과\n",
        "\n",
        "- GPU 활용률과 실제 연산 효율의 차이를 정량적으로 이해  \n",
        "- 모델 구조, 배치 크기, 최적화 기법이 효율에 미치는 영향 인식  \n",
        "- MFU 계산을 통해 병목 구간을 찾아 성능 최적화 방향 제시 가능\n"
      ],
      "metadata": {
        "id": "EUzDL9HqLsCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧮 FLOPs (Floating Point Operations) 개념 정리\n",
        "\n",
        "---\n",
        "\n",
        "## 📘 정의\n",
        "**FLOPs(Floating Point Operations)**란  \n",
        "모델이 수행하는 **부동소수점 연산의 총 개수**를 의미합니다.  \n",
        "\n",
        "즉, 신경망이 학습 또는 추론 과정에서  \n",
        "곱셈(`×`), 덧셈(`+`), 나눗셈(`/`), 지수(`exp`) 등의 **실수 연산이 몇 번 수행되었는가**를 정량적으로 나타내는 지표입니다.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 왜 FLOPs가 중요한가?\n",
        "\n",
        "| 항목 | 설명 |\n",
        "|------|------|\n",
        "| **계산 복잡도 지표** | 모델의 연산량을 정량화하여 복잡도를 비교할 수 있음 |\n",
        "| **성능·속도 예측** | FLOPs가 많을수록 GPU 연산량과 실행 시간이 증가함 |\n",
        "| **효율성 판단 기준** | FLOPs가 적을수록 계산 효율이 높고, 경량 모델에 유리함 |\n",
        "| **하드웨어 비교 기준** | 서로 다른 GPU/TPU 환경 간 모델 효율을 정량 비교 가능 |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Jm_8zQVaRqyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 💾 부동소수점 표준: IEEE 754 요약\n",
        "\n",
        "---\n",
        "\n",
        "## 1) 개요\n",
        "**IEEE 754**는 컴퓨터에서 실수를 표현·연산하는 국제 표준입니다.  \n",
        "핵심 아이디어는 실수를 **부호(Sign)·지수(Exponent)·가수(Fraction)** 로 나눠 **정규화된 과학적 표기**로 저장한다는 것.\n",
        "\n",
        "실수 값은 (정규수의 경우) 다음으로 해석됩니다:\n",
        "$$\n",
        "(-1)^s \\times (1.f)_2 \\times 2^{\\,e - \\text{bias}}\n",
        "$$\n",
        "- $s$: 부호 비트(0=양수, 1=음수)  \n",
        "- $f$: 가수부(숨겨진 1 포함 전제)  \n",
        "- $e$: 지수부 정수값  \n",
        "- $\\text{bias}$: 형식별 바이어스\n",
        "\n",
        "---\n",
        "\n",
        "## 2) 대표 포맷 (Binary)\n",
        "| 포맷 | 총 비트 | 부호 | 지수 | 가수 | 바이어스 |\n",
        "|---|---:|---:|---:|---:|---:|\n",
        "| **binary32 (float)** | 32 | 1 | 8 | 23 | 127 |\n",
        "| **binary64 (double)** | 64 | 1 | 11 | 52 | 1023 |\n",
        "\n",
        "> 정밀도(유효 십진자리 근사): float ≈ 7자리, double ≈ 15~16자리\n",
        "\n",
        "---\n",
        "\n",
        "## 3) 특수 값 인코딩\n",
        "지수와 가수의 비트 패턴 조합으로 특수 값이 정의됩니다.\n",
        "\n",
        "- **정규수(normal)**: $0 < e < \\text{max}$  \n",
        "  값: $(-1)^s (1.f) 2^{e-\\text{bias}}$\n",
        "- **서브노말(subnormal)**: $e=0$, $f\\neq0$  \n",
        "  값: $(-1)^s (0.f) 2^{1-\\text{bias}}$ → **아주 작은 수의 연속성 보장**\n",
        "- **0**: $e=0$, $f=0$ → `+0`, `-0` 존재 (부호만 다름)\n",
        "- **∞ (무한대)**: $e=\\text{max}$, $f=0$ → `+∞`, `-∞`\n",
        "- **NaN**: $e=\\text{max}$, $f\\neq0$  \n",
        "  - **qNaN**(quiet), **sNaN**(signaling) 구분 — 계산 전파/예외 신호\n",
        "\n",
        "---\n",
        "\n",
        "## 4) 반올림 모드\n",
        "기본은 **ties-to-even(가장 가까운 값, 동점은 짝수로)**. 그 외:\n",
        "- toward $+\\infty$, toward $-\\infty$, toward $0$, ties-away-from-zero (확장)\n",
        "\n",
        "반올림은 **연산, 변환, 저장** 단계에서 적용되어 결과 오차를 결정합니다.\n",
        "\n",
        "---\n",
        "\n",
        "## 5) 핵심 지표\n",
        "- **기계 엡실론 $\\epsilon$**: $1$과 구분되는 가장 작은 값 차이  \n",
        "  - float: $\\epsilon \\approx 2^{-23} \\approx 1.19\\times10^{-7}$  \n",
        "  - double: $\\epsilon \\approx 2^{-52} \\approx 2.22\\times10^{-16}$\n",
        "- **ULP (Unit in the Last Place)**: 인접 표현가능 수 간 간격\n",
        "\n",
        "---\n",
        "\n",
        "## 6) 자주 겪는 현상과 주의점\n",
        "- **이진 표현 불가능**: $0.1_{10}$, $0.2_{10}$ 등은 이진에서 **무한소수** → 근사 저장  \n",
        "  ⇒ `0.1 + 0.2 != 0.3`\n",
        "- **연산 비결합성**: $(a+b)+c \\neq a+(b+c)$ (반올림 순서 영향)  \n",
        "  ⇒ 누적합은 Kahan/Neumaier 보정 합계 기법 고려\n",
        "- **소실/취소(cancellation)**: 비슷한 큰 수의 차 $a-b$에서 유효자리 손실  \n",
        "  ⇒ **안정한 수식 변형** 필요\n",
        "- **서브노말 성능**: 매우 작은 수 영역에서 느려질 수 있음 (denormals-are-zero 옵션 등 하드웨어 종속)\n",
        "\n",
        "---\n",
        "\n",
        "## 7) 예시 (binary32)\n",
        "- `1.0f` → `0x3F800000`  \n",
        "- `-0.0f` → `0x80000000`  \n",
        "- `+∞` → `0x7F800000`, `NaN` → `0x7FC00000`(예)\n",
        "\n",
        "---\n",
        "\n",
        "## 8) 확장 포맷과 십진 포맷\n",
        "- **binary128**(quadruple, 128비트), **bfloat16**(지수 8/가수 7) 등 하드웨어·ML 특화 포맷\n",
        "- **decimal32/64/128**: 십진 반올림/회계 용도, 10진 정밀 보존\n",
        "\n",
        "---\n",
        "\n",
        "## 9) 실무 팁\n",
        "- 비교는 **허용 오차**로: `abs(a-b) <= rtol*max(|a|,|b|) + atol`\n",
        "- 누적합/내적은 **Kahan 합계**, 고정 순서 reduce, 혹은 **higher precision** 사용\n",
        "- I/O는 **형식 지정자**로 자릿수 명시(예: `\"{:.17g}\".format(x)` for double)\n",
        "\n",
        "---\n",
        "\n",
        "## 10) 값 해석 공식 요약\n",
        "- 정규수: $(-1)^s (1.f) 2^{e-\\text{bias}}$\n",
        "- 서브노말: $(-1)^s (0.f) 2^{1-\\text{bias}}$\n",
        "\n",
        "> 이 두 식만 기억하면 어떤 비트 패턴도 실수값으로 해석할 수 있습니다.\n"
      ],
      "metadata": {
        "id": "NPj2aNImSCQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧮 FC (Fully Connected) 연산에서 FLOPs 계산 원리\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 개념 요약\n",
        "\n",
        "완전연결(FC, Fully Connected) 레이어는  \n",
        "입력 뉴런($N_{in}$)과 출력 뉴런($N_{out}$)이 **모두 연결**된 형태입니다.\n",
        "\n",
        "각 출력 뉴런은 모든 입력 값에 대해 **가중치 곱셈(Multiply)** 과 **누산(Add)** 을 수행하므로  \n",
        "연산량은 다음과 같습니다:\n",
        "\n",
        "$$\n",
        "FLOPs = 2 \\times N_{in} \\times N_{out}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 📘 왜 2를 곱할까?\n",
        "\n",
        "- **1개의 연결선(Weight)** 은 `입력 × 가중치` → **곱셈 연산(1 FLOP)**  \n",
        "- 여러 입력의 곱셈 결과를 모두 더함 → **덧셈 연산(1 FLOP)**  \n",
        "\n",
        "즉,  \n",
        "하나의 weight 연결당 “곱셈 + 덧셈” = **2 FLOPs**\n",
        "\n",
        "이 때문에 완전연결층 전체의 FLOPs는  \n",
        "입력과 출력 뉴런의 개수를 곱하고 ×2를 해줍니다.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 예시\n",
        "\n",
        "예를 들어 다음과 같은 레이어가 있다고 가정합니다.\n",
        "\n",
        "- 입력 뉴런 수: $N_{in} = 4$  \n",
        "- 출력 뉴런 수: $N_{out} = 3$\n",
        "\n",
        "연산량은 다음과 같습니다.\n",
        "\n",
        "$$\n",
        "FLOPs = 2 \\times 4 \\times 3 = 24\n",
        "$$\n",
        "\n",
        "즉,  \n",
        "**곱셈 12번 + 덧셈 12번 = 총 24 FLOPs**\n",
        "\n",
        "---\n",
        "\n",
        "## ⚙️ 실습 코드 예제\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uK9QpZaNNYSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# FC (Linear) 레이어 정의\n",
        "N_in, N_out = 4, 3\n",
        "fc = nn.Linear(N_in, N_out, bias=False)\n",
        "\n",
        "# 입력 데이터 (배치 크기 1)\n",
        "x = torch.randn(1, N_in)\n",
        "y = fc(x)\n",
        "\n",
        "print(\"입력 크기:\", x.shape)\n",
        "print(\"출력 크기:\", y.shape)\n",
        "\n",
        "# FLOPs 계산\n",
        "flops = 2 * N_in * N_out\n",
        "print(f\"이론적 FLOPs: {flops} 회 연산 (곱셈 + 덧셈 포함)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuN05oOoNaCr",
        "outputId": "ff707b7b-272a-4edd-e667-9d27432d74c8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 크기: torch.Size([1, 4])\n",
            "출력 크기: torch.Size([1, 3])\n",
            "이론적 FLOPs: 24 회 연산 (곱셈 + 덧셈 포함)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧮 컨볼루션 연산에서 FLOPs 계산의 의미\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 개념 요약\n",
        "컨볼루션(Convolution)은 **곱셈(Multiply)** 과 **덧셈(Add)** 연산으로 구성된 대표적인 계산 집약적 연산입니다.\n",
        "\n",
        "한 번의 컨볼루션 연산에서 **하나의 출력 픽셀**을 얻기 위해 수행되는 연산은 다음과 같습니다:\n",
        "\n",
        "$$\n",
        "\\text{연산 수} = K_h \\times K_w \\times C_{in}\n",
        "$$\n",
        "\n",
        "즉,  \n",
        "- 커널(필터) 크기 $K_h \\times K_w$ 만큼의 영역을 입력에서 잘라오고,  \n",
        "- 각 위치마다 $C_{in}$ 개의 채널에 대해 곱셈을 수행하고,  \n",
        "- 그 결과를 모두 더해($+$) 최종 출력을 구합니다.\n",
        "\n",
        "곱셈(Multiply)과 덧셈(Add)은 각각 **1 FLOP**으로 계산되므로,  \n",
        "총 **2 FLOPs**가 한 쌍의 곱셈-덧셈 연산에 해당합니다.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 FLOPs 계산 공식\n",
        "\n",
        "$$\n",
        "FLOPs = 2 \\times H_{out} \\times W_{out} \\times K_h \\times K_w \\times C_{in} \\times C_{out}\n",
        "$$\n",
        "\n",
        "| 항목 | 의미 |\n",
        "|------|------|\n",
        "| $H_{out}, W_{out}$ | 출력 Feature Map의 높이, 너비 |\n",
        "| $K_h, K_w$ | 커널(필터)의 높이, 너비 |\n",
        "| $C_{in}$ | 입력 채널 수 |\n",
        "| $C_{out}$ | 출력 채널(필터 개수) 수 |\n",
        "| 2배 | Multiply + Add (곱셈 + 덧셈) 연산을 모두 포함하기 때문 |\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 직관적 예시\n",
        "\n",
        "예를 들어,  \n",
        "- 입력 크기: $(C_{in}, H, W) = (3, 4, 4)$  \n",
        "- 커널 크기: $(K_h, K_w) = (3, 3)$  \n",
        "- 출력 채널 수: $C_{out} = 1$  \n",
        "- 출력 크기: $(H_{out}, W_{out}) = (2, 2)$  \n",
        "\n",
        "이라면,\n",
        "\n",
        "$$\n",
        "FLOPs = 2 \\times 2 \\times 2 \\times 3 \\times 3 \\times 3 \\times 1 = 216\n",
        "$$\n",
        "\n",
        "즉, **216개의 부동소수점 연산**이 1장의 출력 Feature Map을 계산하는 데 사용됩니다.\n",
        "\n",
        "---\n",
        "\n",
        "## ⚙️ 실습 코드 예제\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vOm21i52MdId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# 입력 및 커널 정의\n",
        "x = torch.randn(1, 3, 4, 4)   # (배치, 채널, 높이, 너비)\n",
        "conv = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3, stride=1, padding=0)\n",
        "\n",
        "# 연산 수행\n",
        "y = conv(x)\n",
        "print(\"출력 크기:\", y.shape)\n",
        "\n",
        "# FLOPs 계산 공식 적용\n",
        "H_out, W_out = y.shape[2], y.shape[3]\n",
        "K_h, K_w = conv.kernel_size\n",
        "C_in, C_out = conv.in_channels, conv.out_channels\n",
        "\n",
        "flops = 2 * H_out * W_out * K_h * K_w * C_in * C_out\n",
        "print(f\"이론적 FLOPs: {flops} 회 연산\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYqzGy-OMh7R",
        "outputId": "2561f67c-7e7e-4eca-e712-3e31219ae2fa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "출력 크기: torch.Size([1, 1, 2, 2])\n",
            "이론적 FLOPs: 216 회 연산\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⚙️ MACs (Multiply–Accumulate Operations) 개념 정리\n",
        "\n",
        "---\n",
        "\n",
        "## 📘 정의\n",
        "**MACs (Multiply–Accumulate Operations)**는  \n",
        "딥러닝 연산에서 자주 등장하는 **곱셈(Multiply)** 과 **덧셈(Accumulate)** 연산의 조합을 의미합니다.\n",
        "\n",
        "즉, 하나의 MAC 연산은 다음 수식을 계산합니다:\n",
        "\n",
        "$$\n",
        "y = (a \\times b) + c\n",
        "$$\n",
        "\n",
        "따라서 **1 MAC = 1 곱셈 + 1 덧셈**  \n",
        "즉, **1 MAC ≈ 2 FLOPs** 로 환산할 수 있습니다.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 왜 MACs가 중요한가?\n",
        "\n",
        "| 항목 | 설명 |\n",
        "|------|------|\n",
        "| **연산 효율 지표** | 하드웨어(특히 GPU/TPU)의 실제 연산 단위와 일치 |\n",
        "| **성능 측정 단위** | CNN, RNN, Transformer 모델의 연산량을 평가할 때 주로 사용 |\n",
        "| **모델 최적화 기준** | MACs 감소는 FLOPs 감소와 거의 동일하며, **속도·전력 효율** 향상으로 직결 |\n",
        "| **FPGA·ASIC 설계 기준** | 대부분의 AI 칩은 MAC 기반으로 병렬 연산 구조를 설계함 |\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 수학적 관계\n",
        "\n",
        "$$\n",
        "1\\ \\text{MAC} = 2\\ \\text{FLOPs}\n",
        "$$\n",
        "\n",
        "즉, FLOPs가 $10^9$ (1 GFLOP)이라면  \n",
        "MACs는 $0.5 \\times 10^9$ (0.5 GMAC) 입니다.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧮 컨볼루션 연산 예시\n",
        "\n",
        "컨볼루션 연산은 입력 특징 맵과 필터(커널)를 곱하고 누적합을 계산합니다.\n",
        "\n",
        "$$\n",
        "\\text{MACs} = H_{out} \\times W_{out} \\times K_h \\times K_w \\times C_{in} \\times C_{out}\n",
        "$$\n",
        "\n",
        "| 기호 | 의미 |\n",
        "|------|------|\n",
        "| $H_{out}, W_{out}$ | 출력 Feature Map 크기 |\n",
        "| $K_h, K_w$ | 커널(필터) 크기 |\n",
        "| $C_{in}$ | 입력 채널 수 |\n",
        "| $C_{out}$ | 출력 채널 수 |\n",
        "\n",
        "> 컨볼루션 한 번당 **각 출력 픽셀**을 계산하기 위해  \n",
        "> 모든 입력 채널의 **곱셈-누산(MAC)** 연산이 수행됩니다.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 Fully Connected 연산 예시\n",
        "\n",
        "완전연결층(FC, Linear Layer)의 경우:\n",
        "$$\n",
        "\\text{MACs} = N_{in} \\times N_{out}\n",
        "$$\n",
        "\n",
        "즉, 입력 뉴런($N_{in}$)과 출력 뉴런($N_{out}$)이 모두 연결되어 있다면,  \n",
        "각 연결마다 1 MAC 연산이 수행됩니다.\n",
        "\n",
        "> 예: 입력 4096, 출력 1000 → $4096 \\times 1000 = 4.1M$ MACs\n",
        "\n",
        "---\n",
        "\n",
        "## ⚙️ PyTorch 예제\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OalEbBLdSazB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 간단한 모델 정의\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(3, 16, 3, stride=1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 32, 3, stride=1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(32 * 32 * 32, 10)\n",
        ")\n",
        "\n",
        "x = torch.randn(1, 3, 32, 32)\n",
        "\n",
        "# MACs 계산 예시\n",
        "H_out, W_out = 32, 32\n",
        "K_h, K_w = 3, 3\n",
        "C_in, C_out = 3, 16\n",
        "\n",
        "conv1_macs = H_out * W_out * K_h * K_w * C_in * C_out\n",
        "print(f\"Conv1 MACs: {conv1_macs / 1e6:.2f} MMACs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPv9Ft37TA7S",
        "outputId": "8a6be302-c29e-41ac-c4b6-3ef482a5832b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conv1 MACs: 0.44 MMACs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔢 1 MAC = 2 FLOPs 의 의미\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 핵심 요약\n",
        "딥러닝이나 수치연산에서  \n",
        "**1 MAC(Multiply–Accumulate)** 연산은 다음 두 단계를 수행합니다.\n",
        "\n",
        "1️⃣ **곱셈(Multiply)**  \n",
        "2️⃣ **덧셈(Add or Accumulate)**\n",
        "\n",
        "즉, 하나의 MAC 연산은 다음 수식에 해당합니다:\n",
        "\n",
        "$$\n",
        "y = (a \\times b) + c\n",
        "$$\n",
        "\n",
        "이 과정에서:\n",
        "- `a × b` → **1개의 곱셈 연산 (1 FLOP)**  \n",
        "- `(a × b) + c` → **1개의 덧셈 연산 (1 FLOP)**  \n",
        "\n",
        "따라서 전체적으로는  \n",
        "**1 MAC = 1 Multiply + 1 Add = 2 Floating Point Operations**\n",
        "\n",
        "---\n",
        "\n",
        "## 🧮 수학적 관점\n",
        "$$\n",
        "1\\ \\text{MAC} = 2\\ \\text{FLOPs}\n",
        "$$\n",
        "\n",
        "| 연산 종류 | FLOPs 수 | 설명 |\n",
        "|------------|-----------|------|\n",
        "| 곱셈(Multiply) | 1 | 실수 × 실수 |\n",
        "| 덧셈(Add) | 1 | 곱셈 결과를 누산기(accumulator)에 더함 |\n",
        "| **합계** | **2 FLOPs** | 1 MAC 수행당 2개의 부동소수점 연산 |\n",
        "\n",
        "---\n",
        "\n",
        "## 💡 예시로 이해하기\n",
        "\n",
        "예를 들어,  \n",
        "다음 계산을 수행한다고 가정합시다:\n",
        "\n",
        "$$\n",
        "y = (2.0 \\times 3.0) + 1.0\n",
        "$$\n",
        "\n",
        "| 단계 | 연산 | FLOPs |\n",
        "|------|------|--------|\n",
        "| ① | 2.0 × 3.0 | 1 FLOP |\n",
        "| ② | (결과 6.0) + 1.0 | 1 FLOP |\n",
        "| **총합** | **1 MAC = 2 FLOPs** | |\n",
        "\n",
        "즉, “곱하고 더하는” 1회 연산 = **2개의 부동소수점 연산**으로 구성됩니다.\n",
        "\n",
        "---\n",
        "\n",
        "## ⚙️ 하드웨어적 의미\n",
        "GPU, CPU, TPU 등 대부분의 하드웨어는  \n",
        "**MAC(Multiply–Accumulate)** 연산 단위를 기반으로 병렬 연산을 수행합니다.\n",
        "\n",
        "- 하나의 **MAC 유닛(MAC Unit)** 은 두 수를 곱하고 결과를 누적기(accumulator)에 더함  \n",
        "- 이는 신경망에서 **가중치 × 입력 + 편향** 형태로 자주 사용됨\n",
        "\n",
        "> 💬 예: CNN, FC, RNN, Transformer 모두 내부적으로 “MAC 연산”으로 구성됨\n",
        "\n",
        "---\n",
        "\n",
        "## 📊 FLOPs vs MACs 비교 요약\n",
        "\n",
        "| 구분 | 의미 | 단위 예시 | 관계 |\n",
        "|------|------|------------|------|\n",
        "| **MACs** | 실제 하드웨어의 연산 조합 단위 (Multiply + Add) | GMAC (10⁹ MACs) | — |\n",
        "| **FLOPs** | 부동소수점 연산 횟수 (곱셈 또는 덧셈) | GFLOPs (10⁹ FLOPs) | 1 MAC = 2 FLOPs |\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 결론\n",
        "\n",
        "- **1 MAC = 2 FLOPs** 는  \n",
        "  한 번의 “곱셈 + 덧셈” 조합이 **2개의 부동소수점 연산**을 포함한다는 의미입니다.  \n",
        "- FLOPs는 연산량을 세밀하게 계산할 때 사용되고,  \n",
        "  MACs는 하드웨어 효율(실제 처리 단위)을 표현할 때 사용됩니다.\n",
        "\n",
        "---\n",
        "\n",
        "✅ **요약 문장**\n",
        "> 하나의 MAC 연산은 부동소수점 곱셈 1회와 덧셈 1회로 이루어지며,  \n",
        "> 따라서 **1 MAC = 2 FLOPs** 로 표현됩니다.\n"
      ],
      "metadata": {
        "id": "uN-OF1zcS8cU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 실습예제 1-3) MFU 측정 도구 사용법"
      ],
      "metadata": {
        "id": "SWd2l6Pz21M4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔍 PyTorch Profiler 안내서\n",
        "\n",
        "`PyTorch Profiler`는 **모델 학습·추론 중 CPU/GPU 연산, 커널, 메모리, I/O** 등을 정밀하게 측정해\n",
        "**병목(bottleneck)** 을 찾아내는 도구입니다. 타임라인과 통계 테이블을 제공하며\n",
        "**TensorBoard**와 연동해 시각화할 수 있습니다.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 무엇을 측정하나요?\n",
        "- **연산 시간**: op별/레이어별 `self_cpu_time_total`, `self_cuda_time_total`\n",
        "- **호출 횟수 & 입력 shape**: `count`, `record_shapes`\n",
        "- **CUDA 커널/스트림 타임라인**: launch 간격, 동기화 지연\n",
        "- **메모리/파라미터(옵션)**: `profile_memory=True`\n",
        "- **데이터 로딩**: `DataLoader::get_batch`, `aten::copy_` 등 I/O 비용\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "9IHES7nbQa1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.profiler as profiler"
      ],
      "metadata": {
        "id": "WQNaBg8S0wnk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [1] 간단한 테스트용 모델 및 입력 데이터\n",
        "model = torch.nn.Linear(1024, 1024).cuda()\n",
        "input_data = torch.randn(16, 1024).cuda()"
      ],
      "metadata": {
        "id": "qNSzO7_r3Ddb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [2] 프로파일링용 기본 설정값\n",
        "batch_size = 16\n",
        "iterations = 50   # 총 반복 횟수"
      ],
      "metadata": {
        "id": "IyGtP8mo3FBT"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [3] torch.profiler로 CPU/GPU 시간 측정\n",
        "with profiler.profile(\n",
        "    activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA],\n",
        "    record_shapes=True\n",
        ") as prof:\n",
        "    for i in range(iterations):\n",
        "        output = model(input_data)\n",
        "        loss = output.sum()\n",
        "        loss.backward()\n",
        "        torch.cuda.synchronize()   # GPU 연산 완료 대기 (정확한 시간 측정용)"
      ],
      "metadata": {
        "id": "R4hxnw833I-b"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [4] 프로파일링 결과에서 평균 수행시간 계산\n",
        "events = prof.key_averages()   # 개별 연산별 집계\n",
        "total_cpu_time = sum([e.self_cpu_time_total for e in events]) / 1e6  # (ms → s)\n",
        "time_per_iter = total_cpu_time / iterations\n",
        "print(f\"🕒 반복 1회당 평균 수행시간: {time_per_iter:.6f} 초\")"
      ],
      "metadata": {
        "id": "VpxnlPmX3LO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4ff10c6-14a7-4ad1-ab14-48569007d785"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🕒 반복 1회당 평균 수행시간: 0.001618 초\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [5] 처리량(Throughput) 계산\n",
        "throughput = batch_size / time_per_iter\n",
        "print(f\"⚡ 처리량(Throughput): {throughput:.2f} 샘플/초\")"
      ],
      "metadata": {
        "id": "-3aj3jcR3S27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c71558b-f869-4f37-8fa8-ec0ab9d99af4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚡ 처리량(Throughput): 9890.52 샘플/초\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [6] FLOPs 및 GPU 이론 성능 설정\n",
        "flops_per_sample = 1e9  # 예시: 1 GFLOP / 샘플\n",
        "total_flops = flops_per_sample * batch_size"
      ],
      "metadata": {
        "id": "1Mv7srDd3YAL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NVIDIA A100 FP32 성능 (19.5 TFLOPs)\n",
        "gpu_peak_flops = 19.5 * 1e12  # FP32 기준\n",
        "# 만약 FP16 혼합정밀도(Half precision)라면 → 312 * 1e12 로 변경"
      ],
      "metadata": {
        "id": "h8bZ_8J24W-b"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [7] MFU 계산\n",
        "mfu = (total_flops / time_per_iter) / gpu_peak_flops\n",
        "print(f\"🔥 MFU (Model FLOPs Utilization): {mfu:.2%}\")"
      ],
      "metadata": {
        "id": "qjzQ71jU4ZdC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3741fbe8-d7f0-4fd3-a6b3-ad02de8c8e2d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔥 MFU (Model FLOPs Utilization): 50.72%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prof.export_chrome_trace(\"trace.json\")"
      ],
      "metadata": {
        "id": "DXmZjMnq4bri"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}